{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527b2776-5f42-4907-9b8c-1e376f87f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1a380c-c28e-48b0-82a5-3ca01e1ba188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "app_name = \"SeQura Data Engineer\"\n",
    "master = \"local[8]\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738b1eb-ff4b-45d6-9718-28401570f093",
   "metadata": {},
   "source": [
    "## Load Mock Data into the Landing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf42a15-1891-43ef-ade1-02b136e9d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+----+----+\n",
      "|    Id|record_date|dim1|dim2|dim3|\n",
      "+------+-----------+----+----+----+\n",
      "|Aaaaaa| 2022-01-01|   4|   1|   2|\n",
      "|Bbbbbb| 2022-01-01|   7|   1|   8|\n",
      "|Cccccc| 2022-01-01|   7|   6|   9|\n",
      "|Dddddd| 2022-01-01|   6|   3|   3|\n",
      "|Eeeeee| 2022-01-01|   0|   1|   4|\n",
      "|Ffffff| 2022-01-01|   9|   9|   0|\n",
      "|Gggggg| 2022-01-01|   1|   1|   5|\n",
      "|Hhhhhh| 2022-01-01|   7|   1|   0|\n",
      "|Iiiiii| 2022-01-01|   8|   7|   1|\n",
      "|Jjjjjj| 2022-01-01|   8|   5|   7|\n",
      "|Kkkkkk| 2022-01-01|   1|   7|   7|\n",
      "|Llllll| 2022-01-01|   1|   9|   8|\n",
      "|Mmmmmm| 2022-01-01|   7|   4|   5|\n",
      "|Nnnnnn| 2022-01-01|   2|   7|   2|\n",
      "|Oooooo| 2022-01-01|   9|   9|   9|\n",
      "|Pppppp| 2022-01-01|   9|   7|   5|\n",
      "|Qqqqqq| 2022-01-01|   6|   9|   8|\n",
      "|Rrrrrr| 2022-01-01|   5|   6|   9|\n",
      "|Ssssss| 2022-01-01|   6|   3|   0|\n",
      "|Tttttt| 2022-01-01|   3|   3|   8|\n",
      "+------+-----------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "record_date = date(2022, 1, 1)\n",
    "today = date.today()\n",
    "\n",
    "data = []\n",
    "while record_date < (date.today() - timedelta(days=1)):\n",
    "\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyx':\n",
    "        record_id = f\"{letter*6}\".capitalize()\n",
    "        data.append(\n",
    "            {\"Id\": record_id, \n",
    "             \"record_date\": record_date, \n",
    "             \"dim1\": random.choice(range(10)), \n",
    "             \"dim2\": random.choice(range(10)), \n",
    "             \"dim3\": random.choice(range(10))})\n",
    "\n",
    "    record_date = record_date + timedelta(days=1)\n",
    "    \n",
    "schema = StructType([StructField('Id', StringType(), nullable=False),\n",
    "                     StructField('record_date', DateType(), nullable=False),\n",
    "                     StructField('dim1', IntegerType(), nullable=False),\n",
    "                     StructField('dim2', IntegerType(), nullable=False),\n",
    "                     StructField('dim3', IntegerType(), nullable=False)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f6275f-03af-4528-8f4e-fc851637a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"data/landing/credit-status-snapshots.parquet\"\n",
    "\n",
    "df = df.repartition(\"record_date\")\n",
    "df.write.mode(\"overwrite\").parquet(filename)\n",
    "df.write.partitionBy(\"record_date\").mode(\"overwrite\").parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65946f-9824-43b3-a7bb-6ef20f6d2bd1",
   "metadata": {},
   "source": [
    "## Extract data from the Landing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14609479-a5ab-40d2-ab15-2ce3706c7708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|record_date|count(1)|\n",
      "+-----------+--------+\n",
      "| 2022-01-01|      26|\n",
      "| 2022-01-02|      26|\n",
      "| 2022-01-03|      26|\n",
      "| 2022-01-04|      26|\n",
      "| 2022-01-05|      26|\n",
      "| 2022-01-06|      26|\n",
      "| 2022-01-07|      26|\n",
      "| 2022-01-08|      26|\n",
      "| 2022-01-09|      26|\n",
      "| 2022-01-10|      26|\n",
      "| 2022-01-11|      26|\n",
      "| 2022-01-12|      26|\n",
      "| 2022-01-13|      26|\n",
      "| 2022-01-14|      26|\n",
      "| 2022-01-15|      26|\n",
      "| 2022-01-16|      26|\n",
      "| 2022-01-17|      26|\n",
      "| 2022-01-18|      26|\n",
      "| 2022-01-19|      26|\n",
      "| 2022-01-20|      26|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = f\"data/landing/credit-status-snapshots.parquet\"\n",
    "\n",
    "agg_df = spark.read.option(\"basePath\", f\"{filename}/\").parquet(f\"{filename}/record_date=*\")\n",
    "# agg_df.show()\n",
    "\n",
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "table_name = f\"LandingAggregateTable{today}\"\n",
    "agg_df.createOrReplaceTempView(table_name)\n",
    "\n",
    "spark.sql(f\"select distinct(record_date), count(1) from {table_name} group by 1 order by 1 asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6a3a8-577c-47f1-be89-4cef6e3acec6",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18314c7b-cce6-423f-a8fd-38e9559caa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "\n",
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "table_name = f\"LandingAggregateTable{today}\"\n",
    "\n",
    "day = spark.sql(f\"select min(record_date) from {table_name}\").collect()[0][0]\n",
    "day_before = None\n",
    "while day <= date.today():\n",
    "    if day == day.replace(day=1):\n",
    "        if day_before:\n",
    "            days.append(day_before)\n",
    "    \n",
    "    day_before = day\n",
    "    day += timedelta(days=1)\n",
    "\n",
    "day = spark.sql(f\"select max(record_date) from {table_name}\").collect()[0][0]\n",
    "\n",
    "if day not in days:\n",
    "    days.append(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99ea01c-f8f9-485e-bf20-50e58970a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|record_date|count(1)|\n",
      "+-----------+--------+\n",
      "| 2022-01-31|      26|\n",
      "| 2022-02-28|      26|\n",
      "| 2022-03-25|      26|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = agg_df.where(agg_df.record_date.isin(days))\n",
    "# new_df.show()\n",
    "\n",
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "table_name = f\"ProcessedAggregateTable{today}\"\n",
    "new_df.createOrReplaceTempView(table_name)\n",
    "\n",
    "spark.sql(f\"select distinct(record_date), count(1) from {table_name} group by 1 order by 1 asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f2ac2-1fc5-4e86-a4ea-af995c768f0b",
   "metadata": {},
   "source": [
    "## Load data into the Processed Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6711f3ea-f6a0-460e-a5e0-83613e12e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"data/processed/credit-status-snapshots-{today}.parquet\"\n",
    "\n",
    "new_df.write.partitionBy(\"record_date\").mode(\"overwrite\").parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa18e1b-0d01-438b-ac74-e92a918394d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00e12e-4a74-49da-8758-c5f8df782f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
